{
    "docs": [
        {
            "location": "/", 
            "text": "Pisces\n\n\nA practical seismological database library in Python.\n\n\n\n\nOverview\n\n\nPisces connects your Python analysis environment to a seismological database.\n\n\nManage and analyze data in the same language\n\nDon't use separate data-management language, like SQL or shell scripts. Just use Python, and connect to \nSciPy\n, \nObsPy\n, \nAIMBAT\n, \npyTDMT\n, \nStreamPick\n, and the rest of the scientific Python ecosystem.\n\n\nUse common open-source technologies and standards\n\nSQL relational databases, \nPython\n, \nSQLAlchemy\n, and the \nSciPy stack\n are widely-used, free, and open-source technologies.\nBecause of this, you can leverage knowledge from sites, like \nStackOverflow\n, and other disciplines, like web development, for database examples, troubleshooting, or tricks.\n\n\nWrite portable, extensible, and scalable code\n\nPython is multi-platform, SQLAlchemy is database-agnostic, and the whole stack is free and open-source.  Write code that will not eventually have to be abandoned due to project size, system architecture, or budgetary or licensing concerns.\n\n\nFeatures\n\n\n\n\nImport/export waveforms directly to/from your database.\n\n\nEasy importing/exporting of text \"flat-file\" data tables.\n\n\nBuild database queries using Python objects and methods (\nSQLAlchemy\n), not by concatenating SQL strings.\n\n\nIntegration with \nObsPy\n.\n\n\nGeographic filtering of results.\n\n\n\n\n\n\nWhat does it look like?\n\n\nDefine tables\n\n\nName your Center for Seismic Studies (CSS) 3.0 tables in a module (e.g. mytables.py),\ninheriting structure and constraints.\nThis just needs to be done once per table name.\n\n\nmytables.py\n\n\nimport pisces.schema.css3 as css\n\nclass Affiliation(css.Affiliation):\n    __tablename__ = 'affiliation'\n\nclass Site(css.Site):\n    __tablename__ = 'site'\n\nclass Origin(css.Origin):\n    __tablename__ = 'origin'\n\nclass Wfdisc(css.Wfdisc):\n    __tablename__ = 'Wfdisc'\n\n\n\nImporting tables\n\n\nImport your tables.\n\n\nfrom mytables import Site, Origin\nfrom mytables import Affiliation as Affil\n\n\n\nImport/reflect arbitrary existing database tables.\n\n\nimport pisces as ps\n\nsession = ps.db_connect('sqlite:///mydb.sqlite')\nsometable, othertable = ps.get_tables(session.bind, ['sometable','othertable'])\n\n\n\nQuerying tables\n\n\nQuery all stations from the CREST seismic deployment, using SQLAlchemy\n\n\nq = session.query(Site).filter(Site.ondate.between(2008001, 2008365))\ncsites = q.filter(Site.sta == Affil.sta).filter(Affil.net == 'XP').all()\n\n\n\nQuery for western US earthquakes, using a Pisces query builder\n\n\nimport pisces.request as req\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)})\n\n\n\nAdd Albuquerque ANMO to the site table, and the Chelyabinsk bolide to the origin table.\n\n\nANMO = Site(sta='ANMO', lat=34.9459, lon=-106.4572, elev=1.85)\nbolide = Origin(orid=1, lat=55.15, lon=61.41, mb=2.7, etype='xm')\nsession.add_all([ANMO, bolide])\nsession.commit()\n\n\n\nEdit a Site, delete an Origin.\n\n\nsession.query(Site).filter(Site.sta == 'MK31').update({'lat': 42.5})\nsession.query(Origin).filter(Origin.orid = 1001).delete()\nsession.commit()\nsession.close()\n\n\n\nGet a waveform\n\n\nGet an ObsPy \nTrace\n object from your waveform description (wfdisc) table.\n\n\nfrom mytables import Wfdisc\n\nwf = session.query(Wfdisc).filter(Wfdisc.sta == 'ANMO').first()  \ntr = wf.to_trace()  \ntr.plot()\n\n\n\n\n\n\n\nInstallation\n\n\nRequires:\n\n\n\n\nNumPy\n\n\nObsPy\n\n\nSQLAlchemy\n0.7\n\n\nC, Fortran compiler\n\n\n\n\nInstall from \nPyPI\n:\n\n\npip install pisces-db\n\n\n\nInstall current master from GitHub:\n\n\npip install git+https://github.com/jkmacc-LANL/pisces", 
            "title": "Home"
        }, 
        {
            "location": "/#pisces", 
            "text": "A practical seismological database library in Python.", 
            "title": "Pisces"
        }, 
        {
            "location": "/#overview", 
            "text": "Pisces connects your Python analysis environment to a seismological database.  Manage and analyze data in the same language \nDon't use separate data-management language, like SQL or shell scripts. Just use Python, and connect to  SciPy ,  ObsPy ,  AIMBAT ,  pyTDMT ,  StreamPick , and the rest of the scientific Python ecosystem.  Use common open-source technologies and standards \nSQL relational databases,  Python ,  SQLAlchemy , and the  SciPy stack  are widely-used, free, and open-source technologies.\nBecause of this, you can leverage knowledge from sites, like  StackOverflow , and other disciplines, like web development, for database examples, troubleshooting, or tricks.  Write portable, extensible, and scalable code \nPython is multi-platform, SQLAlchemy is database-agnostic, and the whole stack is free and open-source.  Write code that will not eventually have to be abandoned due to project size, system architecture, or budgetary or licensing concerns.", 
            "title": "Overview"
        }, 
        {
            "location": "/#features", 
            "text": "Import/export waveforms directly to/from your database.  Easy importing/exporting of text \"flat-file\" data tables.  Build database queries using Python objects and methods ( SQLAlchemy ), not by concatenating SQL strings.  Integration with  ObsPy .  Geographic filtering of results.", 
            "title": "Features"
        }, 
        {
            "location": "/#what-does-it-look-like", 
            "text": "", 
            "title": "What does it look like?"
        }, 
        {
            "location": "/#define-tables", 
            "text": "Name your Center for Seismic Studies (CSS) 3.0 tables in a module (e.g. mytables.py),\ninheriting structure and constraints.\nThis just needs to be done once per table name.", 
            "title": "Define tables"
        }, 
        {
            "location": "/#mytablespy", 
            "text": "import pisces.schema.css3 as css\n\nclass Affiliation(css.Affiliation):\n    __tablename__ = 'affiliation'\n\nclass Site(css.Site):\n    __tablename__ = 'site'\n\nclass Origin(css.Origin):\n    __tablename__ = 'origin'\n\nclass Wfdisc(css.Wfdisc):\n    __tablename__ = 'Wfdisc'", 
            "title": "mytables.py"
        }, 
        {
            "location": "/#importing-tables", 
            "text": "Import your tables.  from mytables import Site, Origin\nfrom mytables import Affiliation as Affil  Import/reflect arbitrary existing database tables.  import pisces as ps\n\nsession = ps.db_connect('sqlite:///mydb.sqlite')\nsometable, othertable = ps.get_tables(session.bind, ['sometable','othertable'])", 
            "title": "Importing tables"
        }, 
        {
            "location": "/#querying-tables", 
            "text": "Query all stations from the CREST seismic deployment, using SQLAlchemy  q = session.query(Site).filter(Site.ondate.between(2008001, 2008365))\ncsites = q.filter(Site.sta == Affil.sta).filter(Affil.net == 'XP').all()  Query for western US earthquakes, using a Pisces query builder  import pisces.request as req\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)})  Add Albuquerque ANMO to the site table, and the Chelyabinsk bolide to the origin table.  ANMO = Site(sta='ANMO', lat=34.9459, lon=-106.4572, elev=1.85)\nbolide = Origin(orid=1, lat=55.15, lon=61.41, mb=2.7, etype='xm')\nsession.add_all([ANMO, bolide])\nsession.commit()  Edit a Site, delete an Origin.  session.query(Site).filter(Site.sta == 'MK31').update({'lat': 42.5})\nsession.query(Origin).filter(Origin.orid = 1001).delete()\nsession.commit()\nsession.close()", 
            "title": "Querying tables"
        }, 
        {
            "location": "/#get-a-waveform", 
            "text": "Get an ObsPy  Trace  object from your waveform description (wfdisc) table.  from mytables import Wfdisc\n\nwf = session.query(Wfdisc).filter(Wfdisc.sta == 'ANMO').first()  \ntr = wf.to_trace()  \ntr.plot()", 
            "title": "Get a waveform"
        }, 
        {
            "location": "/#installation", 
            "text": "Requires:   NumPy  ObsPy  SQLAlchemy 0.7  C, Fortran compiler   Install from  PyPI :  pip install pisces-db  Install current master from GitHub:  pip install git+https://github.com/jkmacc-LANL/pisces", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorial.tables/", 
            "text": "The Database\n\n\nLearn about the tables, how to create them, and how to make new tables.\n\n\n\n\nIntroduction\n\n\nPisces uses relational database tables to represent station, event, and waveform information.\nOne can think of database tables as a collection of inter-related sheets in an Excel spreadsheet, each holding columns and rows of data.\nThe specific tables Pisces uses are defined in the Center for Seismic Studies (CSS) 3.0 seismic schema, and implemented in an SQL relational database.\n\n\nSQL databases are ubiquitous, and describing them in detail is beyond the scope of this tutorial.\nFor further guidance or tutorials, please consult the \nweb\n.\nBefore continuing, however, here are two things to remember:\n\n\n\n\n\n\nYou don't have to write SQL.\n\n\nPisces uses the \nSQLAlchemy\n package (SQLA) and its \nObject Relational Mapper\n to represent database tables.\n\n\n\n\nThe SQLAlchemy Object Relational Mapper presents a method of associating user-defined Python classes with database tables, and instances of those classes (objects) with rows in their corresponding tables. --\nMike Beyer\n, creator SQLAlchemy\n\n\n\n\nIn other words, we can work with familiar Python concepts (classes, instances, and methods) instead of writing SQL (though you can still write SQL if you want).\nThis doesn't mean you can \nignore\n the SQL-ness of your database.\nYou should understand the concepts, you just don't have to write and manage SQL strings.\n\n\n\n\n\n\nYou don't have to install a relational database management system (DBMS).\n  \n\n\nYou already have one.\n\nSQLite\n is part of the Python Standard Library, and SQLAlchemy is compatible with SQLite.\nSQLA is also compatible with \nmany\n other database backends, so you could scale up and use Oracle or PostgreSQL if it makes sense for your project, \nand you wouldn't have to change your code\n.\n\n\n\n\n\n\n\n\n\n\n\nThe core tables\n\n\nPisces comes with the 20 prototype tables defined in the CSS 3.0 seismic schema.\nThese are described in detail in the original specification (Anderson et al., 1990).\nBelow are the entity-relationship diagrams for the CSS 3.0 schema from Anderson et al. (1990).\n\n\n\n\n\nPrimary tables\n\n\n(only primary and unique keys are shown)\n\n\n\n\nLookup tables\n\n\n(only primary and unique keys are shown)\n\n\n\n\nAnderson, J., Farrell, W. E., Garcia, K., Given, J., and Swanger, H. (1990). Center for Seismic Studies version 3 database: Schema reference manual. Technical Report C90-01, Center for Seismic Studies, 1300 N. 17th Street, Suite 1450, Arlington, Virginia 22209-3871. \nPDF (19 MB)\n\n\nThe Pisces implementation\n\n\nPisces implements CSS 3.0 schema with relatively few constraints:\n\n\n\n\nPrimary keys and unique keys are defined, but foreign keys are not. All joins must be explicit.\n\n\nids (e.g. orid, wfid) are not automatically incremented. You must use a counter and the \"lastid\" table.\n\n\nThere are no indexes (internal database structures that speed up queries on certain columns or sets of columns).\n\n\nAny column can take an NA value, which will be assigned if not otherwise specified.\n\n\n\n\nThis is done to offer users the flexibility to maintain a database to a level of rigor commensurate with the project.\nA single-user with a 2-year project database may only ever use the origin, wfdisc, arrival, and assoc tables, and doesn't want to deal with foreign keys,\nwhereas a multi-user global database may need to populate most/all tables and use consistent ids.\nBoth are possible.\n\n\n\n\n\n\n\nExtending the schema\n\n\nDefining new tables\n\n\nThe core tables don't describe everything you might care about, such as stacked cross-correlation functions.\nHere we'll reproduce the \"ccwfdisc\" cross-correlation descriptor table from \nhttp://www.iris.edu/dms/products/ancc-ciei/\n, which looks like this \n(sic)\n:\n\n\n\n\nWe'll define our new table in a file \"mytables.py\" by doing the following:\n\n\n\n\nImport our css3 prototypes into the \ncss\n namespace.\n\n\nInherit from \ncss.Base\n, so that the Ccwfdisc table is a proper SQLAlchemy table, \n  and so that the \ninfo=\n dictionaries are used properly in Pisces\n\n\nName the table with \n__tablename__\n\n\nSpecify the primary and unique constraints with \n__table_args__\n.\n  Each table needs at least a primary key constraint.\n\n\nRe-use a number of known columns with \n.copy()\n\n\nDefine new columns, including the \ninfo\n dictionary for Pisces.\n\n\n\n\nmytables.py\n\n\nimport sqlalchemy as sa\nimport pisces.schema.css3 as css\n\nclass Ccwfdisc(css.Base):\n    __tablename__ = 'ccwfdisc'\n    __table_args__ = (sa.PrimaryKeyConstraint('wfid'), \n                      sa.UniqueConstraint('wfid', 'dir', 'dfile'))\n\n    sta1 = css.sta.copy()\n    net1 = css.net.copy()\n    sta2 = css.sta.copy()\n    net2 = css.net.copy()\n    chan1 = css.chan.copy()\n    chan2 = css.chan.copy()\n    time = css.time.copy()\n    wfid = css.wfid.copy()\n    endtime = css.endtime.copy()\n    nsamp = css.nsamp.copy()\n    samprate = css.samprate.copy()\n    snrn = sa.Column('snrn', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    snrp = sa.Column('snrp', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    sdate = sa.Column('sdate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    edate = sa.Column('edate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    stdays = sa.Column('stdays', sa.Integer,\n        info={'default': -1, 'parse': int, 'width': 6, 'format': '6d'))\n    range = sa.Column('range', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 10, 'format': '10.3f')\n    tsnorm = sa.Column('tsnorm', sa.Float(53), \n        info={'default': -1.0 ,'parse': float, 'width': 14, 'format': '14.4f')\n    datatype = css.datatype.copy()\n    dir = css.dir.copy()\n    dfile = css.dfile.copy()\n    foff = css.foff.copy()\n    lddate = css.lddate.copy()\n\n\n\nColumn info\n\n\nThe \ninfo\n dictionary for each column needs the following entries, at minimum:\n\n\n\n\ndefault\n: The value used for the column when it is otherwise not specified.\n\n\nparse\n: A callable that accepts the fixed-width string and returns the right type for the database.\n\n\nformat\n: The column's external format, from the Python \nstring format specification mini-language\n.\n\n\n\n\nThere are some differences between the Fortran-style \"external format\" and those used by Python.\nIn Python, the type characters may be different from Fortran, and they always follow the width.\nSee the following table for a few examples:\n\n\n\n\n\n\n\n\nDescription\n\n\nFortran\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n6-character string\n\n\na6\n\n\n6.6s\n\n\n\n\n\n\n8-digit integer\n\n\ni8\n\n\n8d\n\n\n\n\n\n\n\n\nCreate a table\n\n\nTo create the table in a database, just use SQLAlchemy syntax:\n\n\nimport sqlalchemy as sa\nfrom mytables import Ccwfdisc\n\nengine = sa.create_engine('sqlite:///mydatabase.sqlite')\n\nCcwfdisc.__table__.create(engine)\n\n\n\nFor every ORM class, there is a hidden \n__table__\n object that has a \ncreate\n method.\nThis method accepts an \nEngine\n instance that points to a specific database, where it will be created.\n\n\nDefining new prototype tables\n\n\nYou can use the previous table with any database, as long as the table will be called \"ccwfdisc\".\nIf you want to re-use this table structure across multiple table names without having to repeat the previous definition each time, you'll need to define a new \nprototype table\n (\nabstract table\n in SQLAlchemy).\n\n\nThis is how the \"ccwfdisc\" table would be defined, as a new prototype.\n\n\nmytables_abs.py\n\n\nimport sqlalchemy as sa\nfrom sqlalchemy.ext.declarative import declared_attr\nimport pisces.schema.css3 as css\n\nclass Ccwfdisc(css.Base):\n    __abstract__ = True\n\n    @declared_attr\n    def __table_args__(cls):\n        return  (sa.PrimaryKeyConstraint('wfid'), \n                 sa.UniqueConstraint('wfid', 'dir', 'dfile'))\n\n    sta1 = css.sta.copy()\n    net1 = css.net.copy()\n    sta2 = css.sta.copy()\n    net2 = css.net.copy()\n    chan1 = css.chan.copy()\n    chan2 = css.chan.copy()\n    time = css.time.copy()\n    wfid = css.wfid.copy()\n    endtime = css.endtime.copy()\n    nsamp = css.nsamp.copy()\n    samprate = css.samprate.copy()\n    snrn = sa.Column('snrn', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    snrp = sa.Column('snrp', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    sdate = sa.Column('sdate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    edate = sa.Column('edate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    stdays = sa.Column('stdays', sa.Integer,\n        info={'default': -1, 'parse': int, 'width': 6, 'format': '6d'))\n    range = sa.Column('range', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 10, 'format': '10.3f')\n    tsnorm = sa.Column('tsnorm', sa.Float(53), \n        info={'default': -1.0 ,'parse': float, 'width': 14, 'format': '14.4f')\n    datatype = css.datatype.copy()\n    dir = css.dir.copy()\n    dfile = css.dfile.copy()\n    foff = css.foff.copy()\n    lddate = css.lddate.copy()\n\n\n\nThe differences are minor:\n\n\n\n\nuse \n__abstract__ = True\n instead of assigning a \n__tablename__\n\n\nuse the \n@declared_attr\n decorator with a \n__table_args__\n \nfunction\n that \nreturns\n the constraint tuple.\n\n\n\n\nAnd this is how the prototype would be implemented and re-used in two different tables, 'ccwfdisc' and 'TA_ccwfdisc'.\n\n\nmytables2.py\n\n\nimport mytables_abs as ab\n\nclass Ccwfdisc(ab.Ccwfdisc):\n    __tablename__ = 'ccwfdisc'\n\nclass TA_Ccwfdisc(ab.Ccwfdisc):\n    __tablename__ = 'TA_ccwfdisc'\n\n\n\nThese two tables look the same, have different names, and can reside in the same database.", 
            "title": "Database Tables"
        }, 
        {
            "location": "/tutorial.tables/#the-database", 
            "text": "Learn about the tables, how to create them, and how to make new tables.", 
            "title": "The Database"
        }, 
        {
            "location": "/tutorial.tables/#introduction", 
            "text": "Pisces uses relational database tables to represent station, event, and waveform information.\nOne can think of database tables as a collection of inter-related sheets in an Excel spreadsheet, each holding columns and rows of data.\nThe specific tables Pisces uses are defined in the Center for Seismic Studies (CSS) 3.0 seismic schema, and implemented in an SQL relational database.  SQL databases are ubiquitous, and describing them in detail is beyond the scope of this tutorial.\nFor further guidance or tutorials, please consult the  web .\nBefore continuing, however, here are two things to remember:    You don't have to write SQL.  Pisces uses the  SQLAlchemy  package (SQLA) and its  Object Relational Mapper  to represent database tables.   The SQLAlchemy Object Relational Mapper presents a method of associating user-defined Python classes with database tables, and instances of those classes (objects) with rows in their corresponding tables. -- Mike Beyer , creator SQLAlchemy   In other words, we can work with familiar Python concepts (classes, instances, and methods) instead of writing SQL (though you can still write SQL if you want).\nThis doesn't mean you can  ignore  the SQL-ness of your database.\nYou should understand the concepts, you just don't have to write and manage SQL strings.    You don't have to install a relational database management system (DBMS).     You already have one. SQLite  is part of the Python Standard Library, and SQLAlchemy is compatible with SQLite.\nSQLA is also compatible with  many  other database backends, so you could scale up and use Oracle or PostgreSQL if it makes sense for your project,  and you wouldn't have to change your code .", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorial.tables/#the-core-tables", 
            "text": "Pisces comes with the 20 prototype tables defined in the CSS 3.0 seismic schema.\nThese are described in detail in the original specification (Anderson et al., 1990).\nBelow are the entity-relationship diagrams for the CSS 3.0 schema from Anderson et al. (1990).", 
            "title": "The core tables"
        }, 
        {
            "location": "/tutorial.tables/#primary-tables", 
            "text": "(only primary and unique keys are shown)", 
            "title": "Primary tables"
        }, 
        {
            "location": "/tutorial.tables/#lookup-tables", 
            "text": "(only primary and unique keys are shown)   Anderson, J., Farrell, W. E., Garcia, K., Given, J., and Swanger, H. (1990). Center for Seismic Studies version 3 database: Schema reference manual. Technical Report C90-01, Center for Seismic Studies, 1300 N. 17th Street, Suite 1450, Arlington, Virginia 22209-3871.  PDF (19 MB)", 
            "title": "Lookup tables"
        }, 
        {
            "location": "/tutorial.tables/#the-pisces-implementation", 
            "text": "Pisces implements CSS 3.0 schema with relatively few constraints:   Primary keys and unique keys are defined, but foreign keys are not. All joins must be explicit.  ids (e.g. orid, wfid) are not automatically incremented. You must use a counter and the \"lastid\" table.  There are no indexes (internal database structures that speed up queries on certain columns or sets of columns).  Any column can take an NA value, which will be assigned if not otherwise specified.   This is done to offer users the flexibility to maintain a database to a level of rigor commensurate with the project.\nA single-user with a 2-year project database may only ever use the origin, wfdisc, arrival, and assoc tables, and doesn't want to deal with foreign keys,\nwhereas a multi-user global database may need to populate most/all tables and use consistent ids.\nBoth are possible.", 
            "title": "The Pisces implementation"
        }, 
        {
            "location": "/tutorial.tables/#extending-the-schema", 
            "text": "", 
            "title": "Extending the schema"
        }, 
        {
            "location": "/tutorial.tables/#defining-new-tables", 
            "text": "The core tables don't describe everything you might care about, such as stacked cross-correlation functions.\nHere we'll reproduce the \"ccwfdisc\" cross-correlation descriptor table from  http://www.iris.edu/dms/products/ancc-ciei/ , which looks like this  (sic) :   We'll define our new table in a file \"mytables.py\" by doing the following:   Import our css3 prototypes into the  css  namespace.  Inherit from  css.Base , so that the Ccwfdisc table is a proper SQLAlchemy table, \n  and so that the  info=  dictionaries are used properly in Pisces  Name the table with  __tablename__  Specify the primary and unique constraints with  __table_args__ .\n  Each table needs at least a primary key constraint.  Re-use a number of known columns with  .copy()  Define new columns, including the  info  dictionary for Pisces.", 
            "title": "Defining new tables"
        }, 
        {
            "location": "/tutorial.tables/#mytablespy", 
            "text": "import sqlalchemy as sa\nimport pisces.schema.css3 as css\n\nclass Ccwfdisc(css.Base):\n    __tablename__ = 'ccwfdisc'\n    __table_args__ = (sa.PrimaryKeyConstraint('wfid'), \n                      sa.UniqueConstraint('wfid', 'dir', 'dfile'))\n\n    sta1 = css.sta.copy()\n    net1 = css.net.copy()\n    sta2 = css.sta.copy()\n    net2 = css.net.copy()\n    chan1 = css.chan.copy()\n    chan2 = css.chan.copy()\n    time = css.time.copy()\n    wfid = css.wfid.copy()\n    endtime = css.endtime.copy()\n    nsamp = css.nsamp.copy()\n    samprate = css.samprate.copy()\n    snrn = sa.Column('snrn', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    snrp = sa.Column('snrp', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    sdate = sa.Column('sdate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    edate = sa.Column('edate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    stdays = sa.Column('stdays', sa.Integer,\n        info={'default': -1, 'parse': int, 'width': 6, 'format': '6d'))\n    range = sa.Column('range', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 10, 'format': '10.3f')\n    tsnorm = sa.Column('tsnorm', sa.Float(53), \n        info={'default': -1.0 ,'parse': float, 'width': 14, 'format': '14.4f')\n    datatype = css.datatype.copy()\n    dir = css.dir.copy()\n    dfile = css.dfile.copy()\n    foff = css.foff.copy()\n    lddate = css.lddate.copy()", 
            "title": "mytables.py"
        }, 
        {
            "location": "/tutorial.tables/#column-info", 
            "text": "The  info  dictionary for each column needs the following entries, at minimum:   default : The value used for the column when it is otherwise not specified.  parse : A callable that accepts the fixed-width string and returns the right type for the database.  format : The column's external format, from the Python  string format specification mini-language .   There are some differences between the Fortran-style \"external format\" and those used by Python.\nIn Python, the type characters may be different from Fortran, and they always follow the width.\nSee the following table for a few examples:     Description  Fortran  Python      6-character string  a6  6.6s    8-digit integer  i8  8d", 
            "title": "Column info"
        }, 
        {
            "location": "/tutorial.tables/#create-a-table", 
            "text": "To create the table in a database, just use SQLAlchemy syntax:  import sqlalchemy as sa\nfrom mytables import Ccwfdisc\n\nengine = sa.create_engine('sqlite:///mydatabase.sqlite')\n\nCcwfdisc.__table__.create(engine)  For every ORM class, there is a hidden  __table__  object that has a  create  method.\nThis method accepts an  Engine  instance that points to a specific database, where it will be created.", 
            "title": "Create a table"
        }, 
        {
            "location": "/tutorial.tables/#defining-new-prototype-tables", 
            "text": "You can use the previous table with any database, as long as the table will be called \"ccwfdisc\".\nIf you want to re-use this table structure across multiple table names without having to repeat the previous definition each time, you'll need to define a new  prototype table  ( abstract table  in SQLAlchemy).  This is how the \"ccwfdisc\" table would be defined, as a new prototype.", 
            "title": "Defining new prototype tables"
        }, 
        {
            "location": "/tutorial.tables/#mytables_abspy", 
            "text": "import sqlalchemy as sa\nfrom sqlalchemy.ext.declarative import declared_attr\nimport pisces.schema.css3 as css\n\nclass Ccwfdisc(css.Base):\n    __abstract__ = True\n\n    @declared_attr\n    def __table_args__(cls):\n        return  (sa.PrimaryKeyConstraint('wfid'), \n                 sa.UniqueConstraint('wfid', 'dir', 'dfile'))\n\n    sta1 = css.sta.copy()\n    net1 = css.net.copy()\n    sta2 = css.sta.copy()\n    net2 = css.net.copy()\n    chan1 = css.chan.copy()\n    chan2 = css.chan.copy()\n    time = css.time.copy()\n    wfid = css.wfid.copy()\n    endtime = css.endtime.copy()\n    nsamp = css.nsamp.copy()\n    samprate = css.samprate.copy()\n    snrn = sa.Column('snrn', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    snrp = sa.Column('snrp', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 16, 'format': '16.6f')\n    sdate = sa.Column('sdate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    edate = sa.Column('edate', sa.Integer, \n        info={'default': -1 ,'parse': int, 'width': 8, 'format': '8d')\n    stdays = sa.Column('stdays', sa.Integer,\n        info={'default': -1, 'parse': int, 'width': 6, 'format': '6d'))\n    range = sa.Column('range', sa.Float(24), \n        info={'default': -1.0 ,'parse': float, 'width': 10, 'format': '10.3f')\n    tsnorm = sa.Column('tsnorm', sa.Float(53), \n        info={'default': -1.0 ,'parse': float, 'width': 14, 'format': '14.4f')\n    datatype = css.datatype.copy()\n    dir = css.dir.copy()\n    dfile = css.dfile.copy()\n    foff = css.foff.copy()\n    lddate = css.lddate.copy()  The differences are minor:   use  __abstract__ = True  instead of assigning a  __tablename__  use the  @declared_attr  decorator with a  __table_args__   function  that  returns  the constraint tuple.   And this is how the prototype would be implemented and re-used in two different tables, 'ccwfdisc' and 'TA_ccwfdisc'.", 
            "title": "mytables_abs.py"
        }, 
        {
            "location": "/tutorial.tables/#mytables2py", 
            "text": "import mytables_abs as ab\n\nclass Ccwfdisc(ab.Ccwfdisc):\n    __tablename__ = 'ccwfdisc'\n\nclass TA_Ccwfdisc(ab.Ccwfdisc):\n    __tablename__ = 'TA_ccwfdisc'  These two tables look the same, have different names, and can reside in the same database.", 
            "title": "mytables2.py"
        }, 
        {
            "location": "/tutorial.queries/", 
            "text": "Using the Database\n\n\nLoad, query, join, write, and copy tables.\n\n\n\n\nIntroduction\n\n\nIn this tutorial, we'll make a database connection, load or import tables, and work with them.\n\n\nTo begin, we'll make a database connection.  \n\n\nimport pisces as ps\n\n# connect with Oracle\nsession = ps.db_connect(user='scott', backend='oracle', server='my.server.edu', \n                        port=1521, instance='mydb')\n\n# connect with sqlite \nsession = ps.db_connect(backend='sqlite', instance='/path/to/mydb.sqlite')\n\n\n\nYou'll need to substitute in your own username, of course.\n\n\ndb_connect\n will prompt you for your password and returns a SQLAlchemy\n\nSession\n\ninstance, which manages database transactions. It can also take a single\nfully-qualified database URL (e.g. \noracle://scott:tiger@my.server.edu:1521/mydb\n or\n\nsqlite:///path/to/mydb.sqlite\n), which is passed to\n\nsqlalchemy.create_engine\n.\nThe returned session is bound to the database through the \n.bind\n attribute,\nwhich is a SQLAlchemy \nEngine\n instance that specifies the database\nlocation and drivers.\n\n\n\n\nLoading Tables\n\n\nTables can be loaded in one of three ways: \n\n\n\n\nImported from your own table definition modules\n\n\nArbitrary tables reflected from the database\n\n\nArbitrary tables reflected from the database, with enhancements\n\n\n\n\nNumber 1 is the recommended way, as it returns a class with useful methods like \nto_trace\n for waveform tables, default value instantiation, and easy flat file conversion.\nNumber 2 returns a class with none of the above enhancements.\nIt is good for quick table loading when you have no prior knowledge of the table or you don't have a table prototype to use for it.\nNumber 3 returns a class with default value instantiation and easy flat file \nwriting\n, but no other Pisces-specific class methods.\n\n\nImport your defined tables\n\n\nImport tables/classes from modules where you previously defined them, as in the \nprevious tutorial page\n.\nThis is the preferred way to load tables, as it offers the most flexibility and functionality.\n\n\nimport from mytables import Site, Wfdisc, Origin\n\n\n\nArbitrary tables\n\n\nIf you only know the table name (and account), you can load a table using \nget_tables\n:\n\n\n# Oracle account.table syntax\nSite, Affil, Origin = ps.get_tables(session.bind, ['user.site', 'user.affiliation', \n                                                   'user.origin'])\n\n# SQLite syntax, no account\nSite, Affil, Origin = ps.get_tables(session.bind, ['site', 'affiliation', 'origin'])\n\n\n\nLoaded tables are just Python classes defined using \nSQLAlchemy\u2019s Object Relational Mapper (ORM)\n.\nTable classes loaded together share a \n.metadata\n.\n\nMetaData\n\nis a Python representation of underlying SQL objects, and can be used to\nissue create/add/drop statements to the database. To add tables loaded\nlater to the same MetaData, use the \nmetadata=\n flag with the existing\ninstance from one of the earlier-loaded tables:\n\n\nWfdisc, Assoc = ps.get_tables(session.bind, ['user.wfdisc', 'user.assoc'], metadata=Site.metadata)\n\n# check the table names in the metadata\nprint Site.metadata.tables.keys()\n\n\n\nTables present in your MetaData can be seen in its \n.tables\n dictionary:\n\n\n['user.affiliation',  'user.site',  'user.wfdisc',  'user.origin',  'user.assoc']\n\n\n\nTables must have a primary key\n\n\nSQLAlchemy\u2019s ORM needs a primary key for the \nsession\n to keep track of rows. If a database table is actually just a view of other tables, or a primary key was never specified,\nwe can specify the primary key in a dictionary and the \nprimary_keys=\n keyword.\n\n\ntry:\n    Sitechan, = ps.get_tables(session.bind, ['user.sitechan'])\nexcept ArgumentError:\n    # failed because user.sitechan has no primary key.  we can force one with primary_keys\n    Sitechan, = ps.get_tables(session.bind, ['user.sitechan'], \n                              primary_keys={'user.sitechan': ['chanid']})\n\n\n\nArbitrary enhanced tables\n\n\nWhen the \nbase=\n option is used with a \nDeclarative Base\n class that comes with Pisces,\nloaded classes get the following enhancements:\n\n\n\n\nThe default values for all fields in a table row (class instance) are filled whether or not the underlying database defines default values.\n\n\nThe class can form its own string representation (flat file row).\n\n\nInstances of the class can iterate over values, and in the correct order.\n\n\n\n\nNote: this will only work if the field names in the table being loaded have been used with this base before.\n\n\n# load table classes, matching column names with those of a known schema\n\nfrom pisces.schema.css3 import Base\n\nSite, Affil, Origin = ps.get_tables(session.bind, ['global.site','global.affiliation', 'global.origin'], base=Base)\n\n\n\nBase\n is the parent class for all pre-defined table prototypes in a given schema that comes with Pisces. \n\n\n\n\nQuerying Tables\n\n\nQuerying and editing tables in Pisces is the same as using SQLAlchemy\u2019s\nORM. Follow SQLAlchemy\u2019s\n\ntutorial\n to\nlearn more about how this works.\n\n\n# query all TA stations installed in 2009\nq = session.query(Site).filter(Site.ondate.between(2009001, 2009365))\nta_sites = q.filter(Site.sta == Affil.sta).filter(Affil.net == 'TA').all()\n\n# query for a subset of western US earthquakes, sorted by mb\nq = session.query(Origin).filter(Origin.lat.between(35, 45)).filter(Origin.mb \n 4)\nwus_quakes = q.filter(Origin.lon.between(-115, -105)).order_by(Origin.mb).all()\n\n# plot 'em\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nm = Basemap(llcrnrlon=-130, llcrnrlat=25, urcrnrlon=-80, urcrnrlat=60, resolution='i')\nm.etopo()\n\nfor ista in ta_sites:\n    x, y = m(ista.lon, ista.lat)\n    m.plot(x, y, 'b^')\n\nfor io in wus_quakes:\n    x, y = m(io.lon, io.lat)\n    m.plot(x, y, 'r*')\n\nplt.title(\"{} TA stations and {} quakes mb \n 4\".format(len(ta_sites), len(wus_quakes)))\n\n\n\n\n\nQuery-builders\n\n\nPisces has a set of intuitive query-building functions for common\nseismic queries in \npisces.request\n.\n\n\nimport pisces.request as req\n\n# repeat the previous query for a subset of western US earthquakes\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)})\n\n# to sort by mb, ask for the query back and do your sort\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)}, asquery=True).order_by(Origin.mb).all()\n\n\n\nPisces uses NumPy/ObsPy to do distance subsets, which are done out-of-database and can be memory intensive. \nThey can be done in-database, if you have a stored function \"xkm_distance\", for example, that calculates lateral distances.\n\n\n# stations \n= 200 km from lat 42, lon -110, out-of-database\nsites = req.get_stations(session, Site, km=(42, -110, 0, 200))\n\n# in-database \"xkm_distance(lat1, lon1, lat2, lon2)\" function\nfrom sqlalchemy import func\nsites = req.get_stations(session, Site, asquery=True).filter(func.xkm_distance(Site.lat, Site.lon, 42, -110).between(0, 200)).all()\n\n\n\nEditing tables\n\n\n# add Albuquerque ANMO and the Chelyabinsk bolide\nANMO = Site(sta='ANMO', lat=34.9459, lon=-106.4572, elev=1.85)\nbolide = Origin(orid=1, lat=55.15, lon=61.41, mb=2.7, etype='xm')\nsession.add_all([ANMO, bolide])\nsession.commit()\n\n# edit a Site, delete an Origin\nsession.query(Site).filter(Site.sta == 'MK31').update({'lat': 42.5})\nsession.query(Origin).filter(Origin.orid = 1001).delete()\nsession.commit()\nsession.close()\n\n\n\nNote that not all fields/attributes were specified when creating \u201cANMO\u201d\nor \u201cbolide\u201d, but their default values are filled in.\n\n\n\n\nUsing ids\n\n\n\n\nCopying Data between Tables", 
            "title": "Using the Database"
        }, 
        {
            "location": "/tutorial.queries/#using-the-database", 
            "text": "Load, query, join, write, and copy tables.", 
            "title": "Using the Database"
        }, 
        {
            "location": "/tutorial.queries/#introduction", 
            "text": "In this tutorial, we'll make a database connection, load or import tables, and work with them.  To begin, we'll make a database connection.    import pisces as ps\n\n# connect with Oracle\nsession = ps.db_connect(user='scott', backend='oracle', server='my.server.edu', \n                        port=1521, instance='mydb')\n\n# connect with sqlite \nsession = ps.db_connect(backend='sqlite', instance='/path/to/mydb.sqlite')  You'll need to substitute in your own username, of course.  db_connect  will prompt you for your password and returns a SQLAlchemy Session \ninstance, which manages database transactions. It can also take a single\nfully-qualified database URL (e.g.  oracle://scott:tiger@my.server.edu:1521/mydb  or sqlite:///path/to/mydb.sqlite ), which is passed to sqlalchemy.create_engine .\nThe returned session is bound to the database through the  .bind  attribute,\nwhich is a SQLAlchemy  Engine  instance that specifies the database\nlocation and drivers.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorial.queries/#loading-tables", 
            "text": "Tables can be loaded in one of three ways:    Imported from your own table definition modules  Arbitrary tables reflected from the database  Arbitrary tables reflected from the database, with enhancements   Number 1 is the recommended way, as it returns a class with useful methods like  to_trace  for waveform tables, default value instantiation, and easy flat file conversion.\nNumber 2 returns a class with none of the above enhancements.\nIt is good for quick table loading when you have no prior knowledge of the table or you don't have a table prototype to use for it.\nNumber 3 returns a class with default value instantiation and easy flat file  writing , but no other Pisces-specific class methods.", 
            "title": "Loading Tables"
        }, 
        {
            "location": "/tutorial.queries/#import-your-defined-tables", 
            "text": "Import tables/classes from modules where you previously defined them, as in the  previous tutorial page .\nThis is the preferred way to load tables, as it offers the most flexibility and functionality.  import from mytables import Site, Wfdisc, Origin", 
            "title": "Import your defined tables"
        }, 
        {
            "location": "/tutorial.queries/#arbitrary-tables", 
            "text": "If you only know the table name (and account), you can load a table using  get_tables :  # Oracle account.table syntax\nSite, Affil, Origin = ps.get_tables(session.bind, ['user.site', 'user.affiliation', \n                                                   'user.origin'])\n\n# SQLite syntax, no account\nSite, Affil, Origin = ps.get_tables(session.bind, ['site', 'affiliation', 'origin'])  Loaded tables are just Python classes defined using  SQLAlchemy\u2019s Object Relational Mapper (ORM) .\nTable classes loaded together share a  .metadata . MetaData \nis a Python representation of underlying SQL objects, and can be used to\nissue create/add/drop statements to the database. To add tables loaded\nlater to the same MetaData, use the  metadata=  flag with the existing\ninstance from one of the earlier-loaded tables:  Wfdisc, Assoc = ps.get_tables(session.bind, ['user.wfdisc', 'user.assoc'], metadata=Site.metadata)\n\n# check the table names in the metadata\nprint Site.metadata.tables.keys()  Tables present in your MetaData can be seen in its  .tables  dictionary:  ['user.affiliation',  'user.site',  'user.wfdisc',  'user.origin',  'user.assoc']", 
            "title": "Arbitrary tables"
        }, 
        {
            "location": "/tutorial.queries/#tables-must-have-a-primary-key", 
            "text": "SQLAlchemy\u2019s ORM needs a primary key for the  session  to keep track of rows. If a database table is actually just a view of other tables, or a primary key was never specified,\nwe can specify the primary key in a dictionary and the  primary_keys=  keyword.  try:\n    Sitechan, = ps.get_tables(session.bind, ['user.sitechan'])\nexcept ArgumentError:\n    # failed because user.sitechan has no primary key.  we can force one with primary_keys\n    Sitechan, = ps.get_tables(session.bind, ['user.sitechan'], \n                              primary_keys={'user.sitechan': ['chanid']})", 
            "title": "Tables must have a primary key"
        }, 
        {
            "location": "/tutorial.queries/#arbitrary-enhanced-tables", 
            "text": "When the  base=  option is used with a  Declarative Base  class that comes with Pisces,\nloaded classes get the following enhancements:   The default values for all fields in a table row (class instance) are filled whether or not the underlying database defines default values.  The class can form its own string representation (flat file row).  Instances of the class can iterate over values, and in the correct order.   Note: this will only work if the field names in the table being loaded have been used with this base before.  # load table classes, matching column names with those of a known schema\n\nfrom pisces.schema.css3 import Base\n\nSite, Affil, Origin = ps.get_tables(session.bind, ['global.site','global.affiliation', 'global.origin'], base=Base)  Base  is the parent class for all pre-defined table prototypes in a given schema that comes with Pisces.", 
            "title": "Arbitrary enhanced tables"
        }, 
        {
            "location": "/tutorial.queries/#querying-tables", 
            "text": "Querying and editing tables in Pisces is the same as using SQLAlchemy\u2019s\nORM. Follow SQLAlchemy\u2019s tutorial  to\nlearn more about how this works.  # query all TA stations installed in 2009\nq = session.query(Site).filter(Site.ondate.between(2009001, 2009365))\nta_sites = q.filter(Site.sta == Affil.sta).filter(Affil.net == 'TA').all()\n\n# query for a subset of western US earthquakes, sorted by mb\nq = session.query(Origin).filter(Origin.lat.between(35, 45)).filter(Origin.mb   4)\nwus_quakes = q.filter(Origin.lon.between(-115, -105)).order_by(Origin.mb).all()\n\n# plot 'em\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nm = Basemap(llcrnrlon=-130, llcrnrlat=25, urcrnrlon=-80, urcrnrlat=60, resolution='i')\nm.etopo()\n\nfor ista in ta_sites:\n    x, y = m(ista.lon, ista.lat)\n    m.plot(x, y, 'b^')\n\nfor io in wus_quakes:\n    x, y = m(io.lon, io.lat)\n    m.plot(x, y, 'r*')\n\nplt.title(\"{} TA stations and {} quakes mb   4\".format(len(ta_sites), len(wus_quakes)))", 
            "title": "Querying Tables"
        }, 
        {
            "location": "/tutorial.queries/#query-builders", 
            "text": "Pisces has a set of intuitive query-building functions for common\nseismic queries in  pisces.request .  import pisces.request as req\n\n# repeat the previous query for a subset of western US earthquakes\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)})\n\n# to sort by mb, ask for the query back and do your sort\nwus_quakes = req.get_events(session, Origin, region=(-115, -105, 35, 45), mag={'mb': (4, None)}, asquery=True).order_by(Origin.mb).all()  Pisces uses NumPy/ObsPy to do distance subsets, which are done out-of-database and can be memory intensive. \nThey can be done in-database, if you have a stored function \"xkm_distance\", for example, that calculates lateral distances.  # stations  = 200 km from lat 42, lon -110, out-of-database\nsites = req.get_stations(session, Site, km=(42, -110, 0, 200))\n\n# in-database \"xkm_distance(lat1, lon1, lat2, lon2)\" function\nfrom sqlalchemy import func\nsites = req.get_stations(session, Site, asquery=True).filter(func.xkm_distance(Site.lat, Site.lon, 42, -110).between(0, 200)).all()", 
            "title": "Query-builders"
        }, 
        {
            "location": "/tutorial.queries/#editing-tables", 
            "text": "# add Albuquerque ANMO and the Chelyabinsk bolide\nANMO = Site(sta='ANMO', lat=34.9459, lon=-106.4572, elev=1.85)\nbolide = Origin(orid=1, lat=55.15, lon=61.41, mb=2.7, etype='xm')\nsession.add_all([ANMO, bolide])\nsession.commit()\n\n# edit a Site, delete an Origin\nsession.query(Site).filter(Site.sta == 'MK31').update({'lat': 42.5})\nsession.query(Origin).filter(Origin.orid = 1001).delete()\nsession.commit()\nsession.close()  Note that not all fields/attributes were specified when creating \u201cANMO\u201d\nor \u201cbolide\u201d, but their default values are filled in.", 
            "title": "Editing tables"
        }, 
        {
            "location": "/tutorial.queries/#using-ids", 
            "text": "", 
            "title": "Using ids"
        }, 
        {
            "location": "/tutorial.queries/#copying-data-between-tables", 
            "text": "", 
            "title": "Copying Data between Tables"
        }, 
        {
            "location": "/tutorial.waveforms/", 
            "text": "Working with Waveforms\n\n\nHow to read and write waveforms.\n\n\n\n\nIntroduction\n\n\n\n\nRetrieving waves from a query\n\n\nWaveforms are described in the \nWfdisc\n table, and there are two ways to get waveforms from a query.\n\n\nRead directly from \nwfdisc\n instances\n\n\nIf you have instances from the \nWfdisc\n class, you can easily convert them to ObsPy \nTrace\n\ninstances for analysis or plotting.  Pisces Wfdisc class instances have a \nto_trace\n method,\nwhich produces \nTrace\n instance from the \nWfdisc\n instance.  Alternately, you can use the \n\nwfdisc2trace\n function on a vanilla SQLAlchemy Wfdisc instance (no \nto_trace\n method).\nFinally, you can use the low-level function \nread_waveform\n, which underlies all the methods above.\nThis function returns a raw NumPy array instead of an Obspy Trace, however.\n\n\nfrom mytables import Wfdisc\nfrom pisces import wfdisc2trace, read_waveform\n\n\n# loop over 10 BHZ wfdisc instances from the database\nfor wf in session.query(Wfdisc).filter(Wfdisc.chan == 'BHZ').limit(10):\n    # the following two traces should be the same\n    tr = wf.to_trace()\n    tr = wfdisc2trace(wf) \n\n    # get the raw data\n    data = read_waveform(wf.dir + '/' + wf.dfile, wf.datatype, wf.foff, wf.nsamp)\n\n    #do analysis, writing, and/or plotting here...\n\n\n\n\n\nUsing the \npisces.Client\n class\n\n\n\n\n\n\n\nAdding waveforms to the database\n\n\nDatabase-building scripts are in development, but adding waveforms to a database\nis still relatively easy.  Using \nObsPy\n, any number of\nwaveform \nformats\n\ncan be read and the basic header \"scraped\" into a \nWfdisc\n row.\n\n\nHere's an example for a SAC file.\n\n\nimport os\nfrom glob import glob\n\nfrom obspy import read\nfrom pisces import db_connect\nfrom pisces.tables.css3 import Wfdisc\n\n\nsession = db_connect(conn='sqlite:///mydatabase.sqlite')\n\nWfdisc.__table__.create(session.bind)\n\nFTYPE = 'SAC'\n\nfor ifile in glob(\n*.SAC\n):\n   tr = read(ifile, format='SAC')[0]\n   idir, idfile = os.path.split(ifile)\n   wf = Wfdisc(sta=tr.stats.station, chan=tr.stats.channel, \n               samprate=tr.stats.sampling_rate, nsamp=tr.stats.npts, \n               time=tr.stats.starttime.timestamp, foff=634, dir=idir,\n               dfile=idfile, endtime=tr.stats.endtime.timestamp)\n   session.add(wf)\n\nsession.commit()\nsession.close()\n\n\n\n\n\n\n\nCopying (localizing) waveform files\n\n\nIf you want to move waveform files from one database to another one, you may need copy the files\nand tweak the Wfdisc table.  Wfdisc tables contain\nfile name and directory information about the waveforms stored on disk, so those need to be \ncorrected when you move the files.  In the example below, we copy wfdisc instances from \nsession1\n,\npointing to the originating database, to \nsession2\n, which points at the destination database.\n\n\nimport os\nimport shutil\n\ndef copy_waves(wfdiscs, old_base, new_base):\n    \nReplace old_base with new_base in the .dir attribute of wfdisc list, and copies the \n    waveform files to the new location.\n\n    Parameters\n    ----------\n    wfdiscs : list\n        Wfdisc instances from source database.\n    old_base, new_base : str\n        The top of the old (new) data directory trees.  This assumes all wfdiscs originate and \n        end up under single directories. The directory structure under the new_base will mirror\n        that under old_base.\n\n    Returns\n    -------\n    wfdiscs_out : list\n        Same as input list, but .dir attribute now points to the new_base location.  \n\n    \n\n    for wf in wfdiscs:\n        # replace the directory\n        old_file =  os.path.sep.join([wf.dir, wf.dfile])\n        wf.dir.replace(old_base, new_base)\n        new_file =  os.path.sep.join([wf.dir, wf.dfile])\n        try:\n            shutil.copyfile(old_file, new_file)\n        except IOError:\n            # new directory doesn't exist yet.  this is like \nmkdir -p\n \n            os.makedirs(os.path.dirname(new_file))\n            shutil.copyfile(old_file, new_file)\n\n# get the wfdiscs\nwfs = session1.query(Wfdisc1).all()\n\n# release the link between the wfdiscs and the originating database\n# this makes them \nfloating\n instances, so they can be added to the destination database\nsession1.expunge_all(wfs)\n\n# do the copy and tweaking\nwfs = copy_waves(wfs, '/old/path/to/top/of/data', '/my/new/path/to/top/of/data')\n\n# add and commit them to the destination database\nsession2.add_all(wfdiscs)\nsession2.commit()", 
            "title": "Import/Export Waveforms"
        }, 
        {
            "location": "/tutorial.waveforms/#working-with-waveforms", 
            "text": "How to read and write waveforms.", 
            "title": "Working with Waveforms"
        }, 
        {
            "location": "/tutorial.waveforms/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorial.waveforms/#retrieving-waves-from-a-query", 
            "text": "Waveforms are described in the  Wfdisc  table, and there are two ways to get waveforms from a query.", 
            "title": "Retrieving waves from a query"
        }, 
        {
            "location": "/tutorial.waveforms/#read-directly-from-wfdisc-instances", 
            "text": "If you have instances from the  Wfdisc  class, you can easily convert them to ObsPy  Trace \ninstances for analysis or plotting.  Pisces Wfdisc class instances have a  to_trace  method,\nwhich produces  Trace  instance from the  Wfdisc  instance.  Alternately, you can use the  wfdisc2trace  function on a vanilla SQLAlchemy Wfdisc instance (no  to_trace  method).\nFinally, you can use the low-level function  read_waveform , which underlies all the methods above.\nThis function returns a raw NumPy array instead of an Obspy Trace, however.  from mytables import Wfdisc\nfrom pisces import wfdisc2trace, read_waveform\n\n\n# loop over 10 BHZ wfdisc instances from the database\nfor wf in session.query(Wfdisc).filter(Wfdisc.chan == 'BHZ').limit(10):\n    # the following two traces should be the same\n    tr = wf.to_trace()\n    tr = wfdisc2trace(wf) \n\n    # get the raw data\n    data = read_waveform(wf.dir + '/' + wf.dfile, wf.datatype, wf.foff, wf.nsamp)\n\n    #do analysis, writing, and/or plotting here...", 
            "title": "Read directly from wfdisc instances"
        }, 
        {
            "location": "/tutorial.waveforms/#using-the-piscesclient-class", 
            "text": "", 
            "title": "Using the pisces.Client class"
        }, 
        {
            "location": "/tutorial.waveforms/#adding-waveforms-to-the-database", 
            "text": "Database-building scripts are in development, but adding waveforms to a database\nis still relatively easy.  Using  ObsPy , any number of\nwaveform  formats \ncan be read and the basic header \"scraped\" into a  Wfdisc  row.  Here's an example for a SAC file.  import os\nfrom glob import glob\n\nfrom obspy import read\nfrom pisces import db_connect\nfrom pisces.tables.css3 import Wfdisc\n\n\nsession = db_connect(conn='sqlite:///mydatabase.sqlite')\n\nWfdisc.__table__.create(session.bind)\n\nFTYPE = 'SAC'\n\nfor ifile in glob( *.SAC ):\n   tr = read(ifile, format='SAC')[0]\n   idir, idfile = os.path.split(ifile)\n   wf = Wfdisc(sta=tr.stats.station, chan=tr.stats.channel, \n               samprate=tr.stats.sampling_rate, nsamp=tr.stats.npts, \n               time=tr.stats.starttime.timestamp, foff=634, dir=idir,\n               dfile=idfile, endtime=tr.stats.endtime.timestamp)\n   session.add(wf)\n\nsession.commit()\nsession.close()", 
            "title": "Adding waveforms to the database"
        }, 
        {
            "location": "/tutorial.waveforms/#copying-localizing-waveform-files", 
            "text": "If you want to move waveform files from one database to another one, you may need copy the files\nand tweak the Wfdisc table.  Wfdisc tables contain\nfile name and directory information about the waveforms stored on disk, so those need to be \ncorrected when you move the files.  In the example below, we copy wfdisc instances from  session1 ,\npointing to the originating database, to  session2 , which points at the destination database.  import os\nimport shutil\n\ndef copy_waves(wfdiscs, old_base, new_base):\n     Replace old_base with new_base in the .dir attribute of wfdisc list, and copies the \n    waveform files to the new location.\n\n    Parameters\n    ----------\n    wfdiscs : list\n        Wfdisc instances from source database.\n    old_base, new_base : str\n        The top of the old (new) data directory trees.  This assumes all wfdiscs originate and \n        end up under single directories. The directory structure under the new_base will mirror\n        that under old_base.\n\n    Returns\n    -------\n    wfdiscs_out : list\n        Same as input list, but .dir attribute now points to the new_base location.  \n\n     \n    for wf in wfdiscs:\n        # replace the directory\n        old_file =  os.path.sep.join([wf.dir, wf.dfile])\n        wf.dir.replace(old_base, new_base)\n        new_file =  os.path.sep.join([wf.dir, wf.dfile])\n        try:\n            shutil.copyfile(old_file, new_file)\n        except IOError:\n            # new directory doesn't exist yet.  this is like  mkdir -p  \n            os.makedirs(os.path.dirname(new_file))\n            shutil.copyfile(old_file, new_file)\n\n# get the wfdiscs\nwfs = session1.query(Wfdisc1).all()\n\n# release the link between the wfdiscs and the originating database\n# this makes them  floating  instances, so they can be added to the destination database\nsession1.expunge_all(wfs)\n\n# do the copy and tweaking\nwfs = copy_waves(wfs, '/old/path/to/top/of/data', '/my/new/path/to/top/of/data')\n\n# add and commit them to the destination database\nsession2.add_all(wfdiscs)\nsession2.commit()", 
            "title": "Copying (localizing) waveform files"
        }, 
        {
            "location": "/tutorial.flatfiles/", 
            "text": "Import/Export Flat Files\n\n\nEasily go between database objects and text files.\n\n\n\n\nImport\n\n\nMany times, database tables are represented as fixed-format text files. \nPisces makes it easy to go between database objects and these external flat file representations.\n\n\nHere, we'll import the following KB Core Site flat file, \"TA.site\".\nEach line in the text file is passed to \nSite.from_string\n, which is a \nPisces-specific method that uses the underlying column descriptions in the class\nto interpret lines in the flat file and to populate a Site row instance. \nFinally, \nsession.add(isite)\n and \nsession.commit()\n add and write rows to the \ndatabase.\n\n\nTA.site\n\n\nK02A         -1  2286324   42.766700 -123.489800    0.9630 Glendale, Oregon,U.S.A.                            ss   K02A      0.0000    0.0000 2009-04-15 15:55:50\nI03D    2009307  2286324   43.697200 -123.348700    0.1400 Drain, OR, USA                                     ss   I03D      0.0000    0.0000 2011-10-11 13:07:17\nP01C         -1  2286324   39.469000 -123.337500    0.4409 Double 8 Ranch, Willits, California,U.S.A.         ss   P01C      0.0000    0.0000 2009-04-15 15:55:50\nN02C         -1  2286324   40.822000 -123.305700    0.7170 Big Bar, California,U.S.A.                         ss   N02C      0.0000    0.0000 2009-04-15 15:55:50\nH03A         -1  2286324   44.676500 -123.292300    0.2143 Soap Creek Ranch, Albany, Oregon,U.S.A.            ss   H03A      0.0000    0.0000 2009-04-15 15:55:50\nG03A         -1  2286324   45.315300 -123.281100    0.2080 Yamhill, Oregon,U.S.A.                             ss   G03A      0.0000    0.0000 2009-04-15 15:55:50\nI03A         -1  2286324   43.972600 -123.277700    0.2057 Eugene, Oregon,U.S.A.                              ss   I03A      0.0000    0.0000 2009-04-15 15:55:50\nG03D    2009297  2286324   45.211500 -123.264100    0.2220 McMinnville, OR, USA                               ss   G03D      0.0000    0.0000 2011-10-11 13:07:17\nD03D    2010237  2286324   47.534700 -123.089400    0.2620 Eldon, WA, USA                                     ss   D03D      0.0000    0.0000 2011-10-11 13:07:17\nF04D    2009318  2286324   46.082900 -123.010800    0.2360 Rainier, OR, USA                                   ss   F04D      0.0000    0.0000 2011-10-11 13:07:17\n\n\n\nRead a flat file Site table into a database\n\n\nimport pisces.schema.kbcore as kb\n\nclass Site(kb.Site):\n    __tablename__ = 'site'\n\nsession = sa.orm.Session(engine)\n\nwith open('TA.site') as ffsite:\n    for line in ffsite:\n        isite = Site.from_string(line)\n        session.add(isite)\nsession.commit()\n\n\n\nExport\n\n\nNext, we'll write database results to a flat file.  These work because the \ninfo\n dictionary in the underlying columns tell the class what the string version of itself should look like.\n\n\nHere, we write 30 origins to a flat file.\n\n\nwith open('TA.origin', 'w') as fforigin:\n    for iorigin in session.query(Origin).filter(Origin.auth == 'REB-IDC').limit(30):\n        fforigin.write(str(iorigin) + os.linesep)\n\n\n\nTA.origin\n\n\n  -6.121700  130.688500    0.0000   954927209.28000    620218    316285  2000096   60   40   -1      280       24 qp      -999.0000 g    5.60    299796    4.30    299797    6.10    299795 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  -4.824500  102.976700   47.7000   954988491.91000    620219    316334  2000097   20   17   -1      274       24 qp      -999.0000 f    4.10    299798    3.20    299799 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  39.195900   24.608700    0.0000   954974602.21000    620220    316319  2000096   16   15   -1      365       30 qp      -999.0000 g    3.80    299801    3.20    299802    3.80    299800 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  22.291000  143.776500  115.8000   954732444.39000    620221    316167  2000094   15   13   -1      213       18 qp      -999.0000 f    3.50    299803 -999.00        -1 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  -9.797000   66.893100    0.0000   954980391.62000    620222    316324  2000097   19   11   -1      429       33 qp      -999.0000 g    4.20    299804    4.10    299805 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n\n\n\nWrite flat files from any combination of columns\n\n\nAd-hoc collections of columns can be also written to well-formed flat files, with the right schema-specific format.\nQueries on specific columns return a list of tuple-like objects called \nKeyedTuples\n, \nwhere values in individual records can be indexed into like a tuple, e.g. \nrecord[0]\n, \nor accessed via attributes, e.g. \nrecord.lat, record.lon\n. \n\n\nLet's write some network-station records to a text file.  First, get the string formatter for the columns you'll be using. \n\nstring_formatter\n returns the correct format string from the \nPython format specification mini-language\n for the columns you'll be writing. \nThe columns must be known to \nBase\n.  That is, each column must have been defined in at least one table that inherited from \nBase\n.\n\n\nfmt = ps.string_formatter(Base.metadata, ['net', 'sta', 'lat', 'lon', 'elev'])\nprint fmt\n\n\n\nfmt\n looks like this:\n\n\n\"{0:8.8s} {1:6.6s} {2:11.6f} {3:11.6f} {4:9.4f}\"\n\n\n\nNow, get the records and write them to file.\n\n\nq = session.query(Affiliation.net, Site.sta, Site.lat, Site.lon, Site.elev)\nq = q.filter(Site.sta == Affiliation.sta)\nq = q.filter(Affiliation.net.in_(['TA', 'UU']))\n\nimport os\nwith open('adhoc.txt', 'w') as f:\n    for netsta in q:\n        f.write(fmt.format(*netsta) + os.linesep)\n\n\n\nadhoc.txt\n\n\nTA       Y53A     33.855400  -83.583600    0.2340\nTA       Y54A     33.862100  -82.688000    0.1760\nTA       Z38A     33.259900  -94.985100    0.1160\nTA       Z49A     33.194200  -86.531100    0.1340\nTA       Z50A     33.254000  -85.922600    0.3700\nTA       Z51A     33.316700  -85.174700    0.2490\nTA       Z52A     33.189300  -84.417600    0.2520\nTA       Z53A     33.280100  -83.571300    0.1440\nTA       Z54A     33.236200  -82.841700    0.1340\nTA       Z55A     33.221100  -82.135900    0.1000\nUU       EOCU     40.777000 -111.899100    1.3560\nUU       QJMH     40.703500 -111.866100    1.3120\nUU       QJOT     40.741600 -111.493900    1.9770\nUU       QKSL     40.377100 -111.861000    1.3790\nUU       QLIN     40.347200 -111.694700    1.5380\nUU       QMDS     40.729000 -111.816100    1.4050\nUU       QNRL     41.740700 -111.824900    1.4070\nUU       QPAY     40.053000 -111.728400    1.4040\nUU       QPML     40.057800 -111.953900    1.3340\nUU       QRJG     40.260900 -111.632700    1.5300\n...", 
            "title": "Import/Export Flat Files"
        }, 
        {
            "location": "/tutorial.flatfiles/#importexport-flat-files", 
            "text": "Easily go between database objects and text files.", 
            "title": "Import/Export Flat Files"
        }, 
        {
            "location": "/tutorial.flatfiles/#import", 
            "text": "Many times, database tables are represented as fixed-format text files. \nPisces makes it easy to go between database objects and these external flat file representations.  Here, we'll import the following KB Core Site flat file, \"TA.site\".\nEach line in the text file is passed to  Site.from_string , which is a \nPisces-specific method that uses the underlying column descriptions in the class\nto interpret lines in the flat file and to populate a Site row instance. \nFinally,  session.add(isite)  and  session.commit()  add and write rows to the \ndatabase.", 
            "title": "Import"
        }, 
        {
            "location": "/tutorial.flatfiles/#tasite", 
            "text": "K02A         -1  2286324   42.766700 -123.489800    0.9630 Glendale, Oregon,U.S.A.                            ss   K02A      0.0000    0.0000 2009-04-15 15:55:50\nI03D    2009307  2286324   43.697200 -123.348700    0.1400 Drain, OR, USA                                     ss   I03D      0.0000    0.0000 2011-10-11 13:07:17\nP01C         -1  2286324   39.469000 -123.337500    0.4409 Double 8 Ranch, Willits, California,U.S.A.         ss   P01C      0.0000    0.0000 2009-04-15 15:55:50\nN02C         -1  2286324   40.822000 -123.305700    0.7170 Big Bar, California,U.S.A.                         ss   N02C      0.0000    0.0000 2009-04-15 15:55:50\nH03A         -1  2286324   44.676500 -123.292300    0.2143 Soap Creek Ranch, Albany, Oregon,U.S.A.            ss   H03A      0.0000    0.0000 2009-04-15 15:55:50\nG03A         -1  2286324   45.315300 -123.281100    0.2080 Yamhill, Oregon,U.S.A.                             ss   G03A      0.0000    0.0000 2009-04-15 15:55:50\nI03A         -1  2286324   43.972600 -123.277700    0.2057 Eugene, Oregon,U.S.A.                              ss   I03A      0.0000    0.0000 2009-04-15 15:55:50\nG03D    2009297  2286324   45.211500 -123.264100    0.2220 McMinnville, OR, USA                               ss   G03D      0.0000    0.0000 2011-10-11 13:07:17\nD03D    2010237  2286324   47.534700 -123.089400    0.2620 Eldon, WA, USA                                     ss   D03D      0.0000    0.0000 2011-10-11 13:07:17\nF04D    2009318  2286324   46.082900 -123.010800    0.2360 Rainier, OR, USA                                   ss   F04D      0.0000    0.0000 2011-10-11 13:07:17  Read a flat file Site table into a database  import pisces.schema.kbcore as kb\n\nclass Site(kb.Site):\n    __tablename__ = 'site'\n\nsession = sa.orm.Session(engine)\n\nwith open('TA.site') as ffsite:\n    for line in ffsite:\n        isite = Site.from_string(line)\n        session.add(isite)\nsession.commit()", 
            "title": "TA.site"
        }, 
        {
            "location": "/tutorial.flatfiles/#export", 
            "text": "Next, we'll write database results to a flat file.  These work because the  info  dictionary in the underlying columns tell the class what the string version of itself should look like.  Here, we write 30 origins to a flat file.  with open('TA.origin', 'w') as fforigin:\n    for iorigin in session.query(Origin).filter(Origin.auth == 'REB-IDC').limit(30):\n        fforigin.write(str(iorigin) + os.linesep)", 
            "title": "Export"
        }, 
        {
            "location": "/tutorial.flatfiles/#taorigin", 
            "text": "-6.121700  130.688500    0.0000   954927209.28000    620218    316285  2000096   60   40   -1      280       24 qp      -999.0000 g    5.60    299796    4.30    299797    6.10    299795 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  -4.824500  102.976700   47.7000   954988491.91000    620219    316334  2000097   20   17   -1      274       24 qp      -999.0000 f    4.10    299798    3.20    299799 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  39.195900   24.608700    0.0000   954974602.21000    620220    316319  2000096   16   15   -1      365       30 qp      -999.0000 g    3.80    299801    3.20    299802    3.80    299800 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  22.291000  143.776500  115.8000   954732444.39000    620221    316167  2000094   15   13   -1      213       18 qp      -999.0000 f    3.50    299803 -999.00        -1 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00\n  -9.797000   66.893100    0.0000   954980391.62000    620222    316324  2000097   19   11   -1      429       33 qp      -999.0000 g    4.20    299804    4.10    299805 -999.00        -1 man:inversion   REB-IDC                     -1 2002-06-11 00:00:00", 
            "title": "TA.origin"
        }, 
        {
            "location": "/tutorial.flatfiles/#write-flat-files-from-any-combination-of-columns", 
            "text": "Ad-hoc collections of columns can be also written to well-formed flat files, with the right schema-specific format.\nQueries on specific columns return a list of tuple-like objects called  KeyedTuples , \nwhere values in individual records can be indexed into like a tuple, e.g.  record[0] , \nor accessed via attributes, e.g.  record.lat, record.lon .   Let's write some network-station records to a text file.  First, get the string formatter for the columns you'll be using.  string_formatter  returns the correct format string from the  Python format specification mini-language  for the columns you'll be writing. \nThe columns must be known to  Base .  That is, each column must have been defined in at least one table that inherited from  Base .  fmt = ps.string_formatter(Base.metadata, ['net', 'sta', 'lat', 'lon', 'elev'])\nprint fmt  fmt  looks like this:  \"{0:8.8s} {1:6.6s} {2:11.6f} {3:11.6f} {4:9.4f}\"  Now, get the records and write them to file.  q = session.query(Affiliation.net, Site.sta, Site.lat, Site.lon, Site.elev)\nq = q.filter(Site.sta == Affiliation.sta)\nq = q.filter(Affiliation.net.in_(['TA', 'UU']))\n\nimport os\nwith open('adhoc.txt', 'w') as f:\n    for netsta in q:\n        f.write(fmt.format(*netsta) + os.linesep)", 
            "title": "Write flat files from any combination of columns"
        }, 
        {
            "location": "/tutorial.flatfiles/#adhoctxt", 
            "text": "TA       Y53A     33.855400  -83.583600    0.2340\nTA       Y54A     33.862100  -82.688000    0.1760\nTA       Z38A     33.259900  -94.985100    0.1160\nTA       Z49A     33.194200  -86.531100    0.1340\nTA       Z50A     33.254000  -85.922600    0.3700\nTA       Z51A     33.316700  -85.174700    0.2490\nTA       Z52A     33.189300  -84.417600    0.2520\nTA       Z53A     33.280100  -83.571300    0.1440\nTA       Z54A     33.236200  -82.841700    0.1340\nTA       Z55A     33.221100  -82.135900    0.1000\nUU       EOCU     40.777000 -111.899100    1.3560\nUU       QJMH     40.703500 -111.866100    1.3120\nUU       QJOT     40.741600 -111.493900    1.9770\nUU       QKSL     40.377100 -111.861000    1.3790\nUU       QLIN     40.347200 -111.694700    1.5380\nUU       QMDS     40.729000 -111.816100    1.4050\nUU       QNRL     41.740700 -111.824900    1.4070\nUU       QPAY     40.053000 -111.728400    1.4040\nUU       QPML     40.057800 -111.953900    1.3340\nUU       QRJG     40.260900 -111.632700    1.5300\n...", 
            "title": "adhoc.txt"
        }, 
        {
            "location": "/api/", 
            "text": "View Pisces API documentation on \nRead the Docs\n.", 
            "title": "API Documentation"
        }, 
        {
            "location": "/about.roadmap/", 
            "text": "Under construction.", 
            "title": "Roadmap"
        }, 
        {
            "location": "/about.license/", 
            "text": "License\n\n\nCopyright 2013. Los Alamos National Security, LLC. This material was produced \nunder U.S. Government contract DE-AC52-06NA25396 for Los Alamos National \nLaboratory (LANL), which is operated by Los Alamos National Security, LLC for\nthe U.S. Department of Energy. The U.S. Government has rights to use, reproduce,\nand distribute this software. NEITHER THE GOVERNMENT NOR LOS ALAMOS NATIONAL\nSECURITY, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY\nFOR THE USE OF THIS SOFTWARE.  If software is modified to produce derivative\nworks, such modified software should be clearly marked, so as not to confuse it\nwith the version available from LANL.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/about.license/#license", 
            "text": "Copyright 2013. Los Alamos National Security, LLC. This material was produced \nunder U.S. Government contract DE-AC52-06NA25396 for Los Alamos National \nLaboratory (LANL), which is operated by Los Alamos National Security, LLC for\nthe U.S. Department of Energy. The U.S. Government has rights to use, reproduce,\nand distribute this software. NEITHER THE GOVERNMENT NOR LOS ALAMOS NATIONAL\nSECURITY, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY\nFOR THE USE OF THIS SOFTWARE.  If software is modified to produce derivative\nworks, such modified software should be clearly marked, so as not to confuse it\nwith the version available from LANL.  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/about.group/", 
            "text": "You can ask questions or search for answers at the \nPisces User's Group\n:\n\n\nhttps://groups.google.com/forum/#!forum/pisces-db\n\n\n\n\n\n\n\n\n\n\n  document.getElementById('forum_embed').src =\n     'https://groups.google.com/forum/embed/?place=forum/pisces-db'\n     + '&showsearch=true&showpopout=true&showtabs=false'\n     + '&parenturl=' + encodeURIComponent(window.location.href);", 
            "title": "User's Group"
        }
    ]
}